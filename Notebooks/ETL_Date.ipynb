{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "import psycopg2 as pg\n",
    "import psycopg2.extras\n",
    "import pandas.io.sql as sqlio\n",
    "\n",
    "import db_connection as dbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension _'Date'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks describes the ETL process to fed the 'Date' dimension table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dimension is essential in almost all data warehousing system, in the sense that all the facts could be observed in a data-time perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case study we will only use dates and not times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dimension _'Date'_ has the following schema:\n",
    "\n",
    "<img src=\"images/date_schema.jpeg\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this project phase, and in order to increase the quality of the date dimension, the following attributes were added:\n",
    "\n",
    "* weekday;\n",
    "* quarter;\n",
    "* semester;\n",
    "* holiday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each record in the date dimension corresponds to a day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of hierarchies that could be identified:\n",
    "\n",
    "* year -> semester -> quarter -> month -> week -> day;\n",
    "* year -> season -> month -> week -> weekday / weekend -> day;\n",
    "* year -> season -> quarter -> month -> weekday -> weekend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will do a small EDA in order to get a sense of the input data of the ETL process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read listings_al.csv, which has the dates needed for the listings fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joao_\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read listings_al.csv data file\n",
    "listings_al_file_path = '../data/listings_al.csv'\n",
    "df_listings = pd.read_csv(listings_al_file_path,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read calendar.csv file, which has the dates needed for the booking fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read calendar.csv data file\n",
    "calendar_file_path = '../data/airbnb/calendar.csv'\n",
    "df_calendar = pd.read_csv(calendar_file_path,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read csv file that contains information about Portugal national holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Given planned earliest start</th>\n",
       "      <th>Given planned earliest end</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Assigned Resources</th>\n",
       "      <th>Additional Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ano Novo</td>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>1/1/2020</td>\n",
       "      <td>feriado nacional em Portugal | feriados.com.pt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Carnaval</td>\n",
       "      <td>2/24/2020</td>\n",
       "      <td>2/24/2020</td>\n",
       "      <td>feriado regional | feriados.com.pt. All data w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sexta-Feira Santa</td>\n",
       "      <td>4/10/2020</td>\n",
       "      <td>4/10/2020</td>\n",
       "      <td>feriado nacional em Portugal | feriados.com.pt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P치scoa</td>\n",
       "      <td>4/12/2020</td>\n",
       "      <td>4/12/2020</td>\n",
       "      <td>feriado nacional em Portugal | feriados.com.pt...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Segunda-feira de P치scoa</td>\n",
       "      <td>4/13/2020</td>\n",
       "      <td>4/13/2020</td>\n",
       "      <td>feriado regional | feriados.com.pt. All data w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Title Given planned earliest start  \\\n",
       "0                 Ano Novo                     1/1/2020   \n",
       "1                 Carnaval                    2/24/2020   \n",
       "2        Sexta-Feira Santa                    4/10/2020   \n",
       "3                   P치scoa                    4/12/2020   \n",
       "4  Segunda-feira de P치scoa                    4/13/2020   \n",
       "\n",
       "  Given planned earliest end  \\\n",
       "0                   1/1/2020   \n",
       "1                  2/24/2020   \n",
       "2                  4/10/2020   \n",
       "3                  4/12/2020   \n",
       "4                  4/13/2020   \n",
       "\n",
       "                                               Notes  Assigned Resources  \\\n",
       "0  feriado nacional em Portugal | feriados.com.pt...                 NaN   \n",
       "1  feriado regional | feriados.com.pt. All data w...                 NaN   \n",
       "2  feriado nacional em Portugal | feriados.com.pt...                 NaN   \n",
       "3  feriado nacional em Portugal | feriados.com.pt...                 NaN   \n",
       "4  feriado regional | feriados.com.pt. All data w...                 NaN   \n",
       "\n",
       "   Additional Title  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "holiday_file_path = '../data/feriados_nacionais.csv'\n",
    "df_holidays = pd.read_csv(holiday_file_path)\n",
    "df_holidays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert dates from string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6976"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(df_listings['host_id'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17168, 147)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert string to date\n",
    "df_listings['DataRegisto'] = [datetime.strptime(d.split('.')[0], \"%Y-%m-%dT%H:%M:%S\") for d in df_listings['DataRegisto']] # split into YYYY-MM-DD HH:MM:SS\n",
    "df_listings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar['date'] = [datetime.strptime(d, \"%Y-%m-%d\") for d in df_calendar['date']] # split into YYYY-MM-DD\n",
    "df_calendar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In _calendar.csv_, there are approximately 365 records for each listing. Each record indicates if the listing is available in that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets drop the duplicates from df_calendar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove df_calendar duplicates and save it new dataframe\n",
    "df_calendar2 = df_calendar.drop_duplicates(subset=['date'])\n",
    "df_calendar2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and process holidays dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_dates = df_holidays.iloc[:,1].values[:-1]\n",
    "holidays_dates = [datetime.strptime(d, \"%m/%d/%Y\") for d in holidays_dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a quick exploration of the data available in _listings_al.csv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count by year\n",
    "years = np.array([d.year for d in df_listings['DataRegisto']]) # makes list of year in each listing's date\n",
    "df_years = pd.DataFrame(years,columns=['year']) # convert to dataframe\n",
    "df_years_graph = df_years.year.value_counts().to_frame('count').reset_index().rename(columns={'index': 'year'}).sort_values('year',ascending = True) # count each year, sort by asc.\n",
    "\n",
    "# remove count of 2020 because it is incomplete (data exported from inside airbnb in january)\n",
    "df_years_graph = df_years_graph[df_years_graph['year']!=2020]\n",
    "\n",
    "# create column with the cumulative sum\n",
    "df_years_graph['cum_count'] = df_years_graph['count'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(df_years_graph['year'],df_years_graph['count'], label = 'New listings')\n",
    "plt.plot(df_years_graph['year'],df_years_graph['cum_count'], label = 'Cumulative new listings')\n",
    "plt.axvline(x = 2008, color = 'red')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of new listings')\n",
    "plt.title('Number of Airbnb listings in Lisbon (all)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems a negligible part of the properties were registered before 2010 for lodging. This already gives us some insights: there was a clear boom in property registration with the intention to host tourists after Airbnb was created in 2008 (red line). Data from 2020 was neglected as it is incomplete (the dataset was scraped in January).\n",
    "\n",
    "Let's plot the data just from 2008 forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_years_graph2010 = df_years_graph[df_years_graph['year']>= 2008]\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(df_years_graph2010['year'],df_years_graph2010['count'], label = 'New listings')\n",
    "plt.plot(df_years_graph2010['year'],df_years_graph2010['cum_count'], label = 'Cumulative new listings')\n",
    "plt.axvline(x = 2008, color = 'red')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of new listings')\n",
    "plt.title('Number of Airbnb listings in Lisbon (2010 - 2019)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that since 2013, the number of new listings in airbnb for Lisbon increased until 2018, from less than 500 new listings per year to more than 5000 new listings per year, which is a 10x increase. In 2019 the growth in new listings decreased compared to the previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_years_graph2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2010 there were 151 listings in Airbnb located in Lisbon, and in 2019 there was a total of 17155 listings, which is an increase of about 113 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sections all the preprocessing will be done in order to get the data cleaned to fed the date dimension table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **pipeline of the ETL process** for the date dimension can be seen in the image above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Pipeline_date.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process receives as inputs 3 csv files:\n",
    "\n",
    "* **'listings_al.csv'**: which contains the dates needed to be inserted in order the fed the listing fact table;\n",
    "* **'calendar.csv'**: contains dates about weather a specific listings is available in the future or not;\n",
    "* **'feriados_nacionais.csv'**: contains all the national holidays of Portugal. This data will be used to fed the 'holiday' columns in this dimension, in order to increase the quality of the analysis that could be done further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This inputs are then transformed and processed during this notebook, in order to insert just the dates that are not in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the process, a csv named 'df_listings_date.csv' is saved in the '/processed_dt/' directory. This csv contains the mapping between the date and the listings facts, and will be used in order to fed the fact tables. We use this same process in all the dimensions due to the way we distribute work among the different elements of the group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline could be used multiple times, because it only takes into account new dates that needed to be inserted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining functions that process each date and build its respective attributes in the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_season(month):\n",
    "    \"\"\"Detects the season of the year the date belongs to\"\"\"\n",
    "    if (month < 3) or (month == 12): return 'Winter'\n",
    "    elif (month >= 3) and (month < 6): return 'Spring'\n",
    "    elif (month >= 6) and (month < 9): return 'Summer'\n",
    "    elif (month >= 9) and (month < 12): return 'Autumn'\n",
    "\n",
    "def date_weekend(week_day):\n",
    "    \"\"\"Detects if date is work day or weekend\"\"\"\n",
    "    if (week_day == 5) or (week_day == 6): return 'Weekend'\n",
    "    else: return 'Work Day'\n",
    "\n",
    "def week_day(d):\n",
    "    \"\"\"Detects weekday\"\"\"\n",
    "    days = {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\", 4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"}\n",
    "    return days[d.weekday()]\n",
    "\n",
    "def date_pk(date):\n",
    "    \"\"\"Builds primary key\"\"\"\n",
    "    return int(date.strftime('%d%m%Y'))\n",
    "\n",
    "def week_of_month(dt):\n",
    "    \"\"\"Detects the week of the month for the specified date\"\"\"\n",
    "    first_day = dt.replace(day=1)\n",
    "    dom = dt.day\n",
    "    adjusted_dom = dom + first_day.weekday()\n",
    "    return int(ceil(adjusted_dom/7.0))\n",
    "\n",
    "def getQuarter(d):\n",
    "    \"\"\"Detects which quarter of the year date belongs to\"\"\"\n",
    "    if d.month<=3: return \"Q1\"\n",
    "    if d.month<=6: return \"Q2\"\n",
    "    if d.month<=9: return \"Q3\"\n",
    "    return \"Q4\"\n",
    "\n",
    "def getSemester(d):\n",
    "    \"\"\"Detects which semester of the year date belongs to\"\"\"\n",
    "    if d.month<=6: return \"S1\"\n",
    "    return \"S2\"\n",
    "\n",
    "def is_holiday(d):\n",
    "    if len(df_holidays2[(df_holidays2['Day']==d.day) & (df_holidays2['Month']==d.month)])>0: return 'Holiday'\n",
    "    else: return 'Not Holiday'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of date dimension primary key in format 'ddMMyyyy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings['DataRegisto'][0], date_pk(df_listings['DataRegisto'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing *feriados_nacionais.csv* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the file in order to get a clean dataframe with the holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe will be used to queries in order to know if a specific date is a national holiday or not. The method is defined above: 'def is_holiday()'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_days = [d.day for d in holidays_dates]\n",
    "holidays_months = [d.month for d in holidays_dates]\n",
    "\n",
    "df_holidays2 = pd.DataFrame(np.stack((holidays_days,holidays_months), axis = 1),columns=['Day','Month'])\n",
    "df_holidays2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in this dataframe will be used in order to fed the 'Holiday' column of the dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the _df_listings_ data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will process the data in order to get the attributes of the date dimension table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pks = np.array([date_pk(d) for d in df_listings['DataRegisto']])\n",
    "df_listings['date_id'] = np.array([date_pk(d) for d in df_listings['DataRegisto']])\n",
    "days = np.array([d.day for d in df_listings['DataRegisto']])\n",
    "weeks = np.array([week_of_month(d) for d in df_listings['DataRegisto']])\n",
    "week_days = np.array([week_day(d) for d in df_listings['DataRegisto']])\n",
    "week_ends = np.array([date_weekend(d.weekday()) for d in df_listings['DataRegisto']])\n",
    "months = np.array([d.month for d in df_listings['DataRegisto']])\n",
    "seasons = np.array([date_season(d.month) for d in df_listings['DataRegisto']])\n",
    "quarters = np.array([getQuarter(d) for d in df_listings['DataRegisto']])\n",
    "semesters = np.array([getSemester(d) for d in df_listings['DataRegisto']])\n",
    "holidays = np.array([is_holiday(d) for d in df_listings['DataRegisto']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing each date in the dataframe and building a list of values for each attribute, we merge them together in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['date_id','day','week', 'month','year','season','weekend','weekday','quarter','semester','holiday']\n",
    "df_date_dimension = pd.DataFrame(np.stack((pks,days,weeks, months,years, seasons,week_ends,week_days,quarters,semesters,holidays),axis=-1), columns = columns)\n",
    "df_date_dimension['date_id'] = df_date_dimension['date_id'].astype(int) \n",
    "df_date_dimension.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_dimension = df_date_dimension.drop_duplicates(subset=['date_id'])\n",
    "df_date_dimension.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the _df_calendar_ data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will process the dates needed in order to feed the bookings fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pks2 = [date_pk(d) for d in df_calendar2['date']]\n",
    "days2 = [d.day for d in df_calendar2['date']]\n",
    "weeks2 = [week_of_month(d) for d in df_calendar2['date']]\n",
    "week_days2 = [week_day(d) for d in df_calendar2['date']]\n",
    "week_ends2 = [date_weekend(d.weekday()) for d in df_calendar2['date']]\n",
    "months2 = [d.month for d in df_calendar2['date']]\n",
    "seasons2 = [date_season(d.month) for d in df_calendar2['date']]\n",
    "quarters2 = [getQuarter(d) for d in df_calendar2['date']]\n",
    "semesters2 = [getSemester(d) for d in df_calendar2['date']]\n",
    "years2 = [d.year for d in df_calendar2['date']]\n",
    "holidays2 = [is_holiday(d) for d in df_calendar2['date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe with dates that came from the 'calendar.csv' input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar_processed = pd.DataFrame(np.stack((pks2,days2,weeks2,months2,years2,seasons2,week_ends2,week_days2,quarters2,semesters2,holidays2),axis=-1), columns = columns)\n",
    "df_calendar_processed['date_id'] = df_calendar_processed['date_id'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ETL output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the ETL process is the dataset with the data available to insert in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets merge the two dataframes (dates needed both for booking and listing facts), in order to get our final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_dimension_insert = pd.concat([df_calendar_processed,df_date_dimension])\n",
    "df_date_dimension_insert['date_id'] = df_date_dimension_insert['date_id'].astype(int)\n",
    "df_date_dimension_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_dimension_insert.to_csv('../processed_dt/dimension_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert data into DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will remove the dates that are both in our ETL output file and in the database, and insert just the new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import database connection settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_connection import dbconnection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_table = \"\"\"\n",
    "DROP TABLE IF EXISTS Date;\n",
    "\"\"\"\n",
    "\n",
    "create_table = \"\"\"\n",
    "CREATE TABLE Date (\n",
    "    DATE_ID INT PRIMARY KEY NOT NULL,\n",
    "    DAY INT NOT NULL,\n",
    "    WEEK INT NOT NULL,\n",
    "    MONTH INT NOT NULL,\n",
    "    YEAR INT NOT NULL,\n",
    "    SEASON VARCHAR(10) CHECK(SEASON in ('Spring', 'Summer', 'Autumn', 'Winter')) NOT NULL,\n",
    "    WEEKEND VARCHAR(11) CHECK (WEEKEND in ('Weekend', 'Work Day')) NOT NULL,\n",
    "    WEEKDAY VARCHAR(10) CHECK (WEEKDAY IN ('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday')) NOT NULL,\n",
    "    QUARTER VARCHAR(2) CHECK(QUARTER IN ('Q1','Q2','Q3','Q4')) NOT NULL,\n",
    "    SEMESTER VARCHAR(2) CHECK(SEMESTER IN ('S1','S2')) NOT NULL,\n",
    "    HOLIDAY VARCHAR(11) CHECK (HOLIDAY in ('Holiday', 'Not Holiday')) NOT NULL\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excuteSingleSQLstatement(sql, host, database, user, password,gssencmode,sslmode):\n",
    "    conn = pg.connect(host=host,database=database, user=user, password=password,sslmode = sslmode,gssencmode=gssencmode)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    cur.close()\n",
    "    conn.commit()\n",
    "    conn.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute just the first time the notebook runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excuteSingleSQLstatement(delete_table, dbconnection.server_host, dbconnection.dbname, dbconnection.dbusername, dbconnection.dbpassword,gssencmode = dbconnection.gssencmode,sslmode=dbconnection.sslmode)\n",
    "#excuteSingleSQLstatement(create_table, dbconnection.server_host, dbconnection.dbname, dbconnection.dbusername, dbconnection.dbpassword,gssencmode = dbconnection.gssencmode,sslmode=dbconnection.sslmode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to query table and convert it to pandas dataframe\n",
    "def query_table(conn, table_name):\n",
    "    \"\"\"Returns DataFrame with queried database table\"\"\"\n",
    "    sql = \"select * from {};\".format(table_name)\n",
    "    #return dataframe\n",
    "    return sqlio.read_sql_query(sql, conn)\n",
    "\n",
    "# for this function to run, the dataframes must have the same columns, in the same order\n",
    "def get_data_to_insert(df_etl, df_sql,pk):\n",
    "    \"\"\"Returns data valid for insertion in dimension from a new ETL-processed DataFrame\"\"\"\n",
    "    df_etl[pk] = df_etl[pk].astype(int)\n",
    "    df_sql[pk] = df_sql[pk].astype(int)\n",
    "    df_insert = df_etl[-df_etl[pk].astype(int).isin(df_sql[pk].astype(int))].dropna(how = 'all')\n",
    "    df_insert = df_insert.drop_duplicates(subset=[pk])\n",
    "    return df_insert\n",
    "\n",
    "# function for bulk insert\n",
    "def insert_data(df, table_name, conn):\n",
    "    \"\"\"Inserts selected data into dimension table in database\"\"\"\n",
    "    df_columns = list(df)\n",
    "    columns = \",\".join(df_columns)\n",
    "    values = \"VALUES({})\".format(\",\".join([\"%s\" for _ in df_columns])) \n",
    "    insert_stmt = \"INSERT INTO {} ({}) {}\".format(table_name,columns,values)\n",
    "    success = True\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        psycopg2.extras.execute_batch(cursor, insert_stmt, df.values)\n",
    "        conn.commit()\n",
    "        success = True\n",
    "    except pg.DatabaseError as error:\n",
    "        success = False\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    return success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query date dimension, from postgres db server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "df_date_dimension_sql = query_table(conn, 'date')\n",
    "conn.close()\n",
    "df_date_dimension_sql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarantee that duplicates will be not be inserted, by getting just the data in dataframe with the ETL output (df_date_dimension_insert) that are not in df_date_dimension_sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insert = get_data_to_insert(df_date_dimension_insert,df_date_dimension_sql,'date_id')\n",
    "df_insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insert data** into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_insert) > 0:\n",
    "    table_name = 'date'\n",
    "    conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "    success = insert_data(df_insert,table_name, conn)\n",
    "    conn.close()\n",
    "    if success == True: print('Data inserted successfully')\n",
    "else: print('No data to insert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe that saves the mapping between the listing_id and date_id. This will be used to load the listings fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = df_listings[['id','date_id']].rename(columns = {'id':'listing_id'})\n",
    "df_mapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save csv to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping.to_csv('../processed_dt/df_listings_date.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
