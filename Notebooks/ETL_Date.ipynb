{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "import psycopg2 as pg\n",
    "import psycopg2.extras\n",
    "import pandas.io.sql as sqlio\n",
    "\n",
    "import db_connection as dbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dimension _'Date'_\n",
    "## 1.1. Reading data into Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read listings_al.csv**, which has the dates needed for the listings fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read listings_al.csv data file\n",
    "listings_al_file_path = '../data/listings_al.csv'\n",
    "df_listings = pd.read_csv(listings_al_file_path,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read calendar.csv file**, which has the dates needed for the availability fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read calendar.csv data file\n",
    "calendar_file_path = '../data/airbnb/calendar.csv'\n",
    "df_calendar = pd.read_csv(calendar_file_path,sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert string to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(df_listings['host_id'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string to date\n",
    "df_listings['DataRegisto'] = [datetime.strptime(d.split('.')[0], \"%Y-%m-%dT%H:%M:%S\") for d in df_listings['DataRegisto']] # split into YYYY-MM-DD HH:MM:SS\n",
    "df_listings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar['date'] = [datetime.strptime(d, \"%Y-%m-%d\") for d in df_calendar['date']] # split into YYYY-MM-DD\n",
    "df_calendar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In _calendar.csv_, there are approximately 365 records for each listing. Each record indicates if the listing is available in that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets drop the duplicates from df_calendar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove df_calendar duplicates and save it new dataframe\n",
    "df_calendar2 = df_calendar.drop_duplicates(subset=['date'])\n",
    "df_calendar2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a quick exploration of the data available in _listings_al.csv_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count by year\n",
    "years = [d.year for d in df_listings['DataRegisto']] # makes list of year in each listing's date\n",
    "df_years = pd.DataFrame(years,columns=['year']) # convert to dataframe\n",
    "df_years_graph = df_years.year.value_counts().to_frame('count').reset_index().rename(columns={'index': 'year'}).sort_values('year',ascending = True) # count each year, sort by asc.\n",
    "\n",
    "# remove count of 2020 because it is incomplete (data exported from inside airbnb in january)\n",
    "df_years_graph = df_years_graph[df_years_graph['year']!=2020]\n",
    "\n",
    "# create column with the cumulative sum\n",
    "df_years_graph['cum_count'] = df_years_graph['count'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(df_years_graph['year'],df_years_graph['count'], label = 'New listings')\n",
    "plt.plot(df_years_graph['year'],df_years_graph['cum_count'], label = 'Cumulative new listings')\n",
    "plt.axvline(x = 2008, color = 'red')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of new listings')\n",
    "plt.title('Number of Airbnb listings in Lisbon (all)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems a negligible part of the properties were registered before 2010 for lodging. This already gives us some insights: there was a clear boom in property registration with the intention to host tourists after Airbnb was created in 2008 (red line). Data from 2020 was neglected as it is incomplete (the dataset was scraped in January).\n",
    "\n",
    "Let's plot the data just from 2010 forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_years_graph2010 = df_years_graph[df_years_graph['year']>= 2008]\n",
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(df_years_graph2010['year'],df_years_graph2010['count'], label = 'New listings')\n",
    "plt.plot(df_years_graph2010['year'],df_years_graph2010['cum_count'], label = 'Cumulative new listings')\n",
    "plt.axvline(x = 2008, color = 'red')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of new listings')\n",
    "plt.title('Number of Airbnb listings in Lisbon (2010 - 2019)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that since 2013, the number of new listings in airbnb for Lisbon increased until 2018, from less than 500 new listings per year to more than 5000 new listings per year, which is a 10x increase. In 2019 the growth in new listings decreased compared to the previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_years_graph2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2010 there were 151 listings in Airbnb located in Lisbon, and in 2019 there was a total of 17155 listings, which is an increase of about 113 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's preprocess the data in order to create our dimension. The dimension _'Date'_ has the following schema:\n",
    "\n",
    "<img src=\"../images/date_schema.png\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "We start by defining functions that process each date and build its respective attributes in the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_season(month):\n",
    "    \"\"\"Detects the season of the year the date belongs to\"\"\"\n",
    "    if (month < 3) or (month == 12): return 'Winter'\n",
    "    elif (month >= 3) and (month < 6): return 'Spring'\n",
    "    elif (month >= 6) and (month < 9): return 'Summer'\n",
    "    elif (month >= 9) and (month < 12): return 'Autumn'\n",
    "\n",
    "def date_weekend(week_day):\n",
    "    \"\"\"Detects if date is work day or weekend\"\"\"\n",
    "    if (week_day == 5) or (week_day == 6): return 'Weekend'\n",
    "    else: return 'Work Day'\n",
    "\n",
    "def week_day(d):\n",
    "    \"\"\"Detects weekday\"\"\"\n",
    "    days = {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\", 4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"}\n",
    "    return days[d.weekday()]\n",
    "\n",
    "def date_pk(date):\n",
    "    \"\"\"Builds primary key\"\"\"\n",
    "    return int(date.strftime('%d%m%Y'))\n",
    "\n",
    "def week_of_month(dt):\n",
    "    \"\"\"Detects the week of the month for the specified date\"\"\"\n",
    "    first_day = dt.replace(day=1)\n",
    "    dom = dt.day\n",
    "    adjusted_dom = dom + first_day.weekday()\n",
    "    return int(ceil(adjusted_dom/7.0))\n",
    "\n",
    "def getQuarter(d):\n",
    "    \"\"\"Detects which quarter of the year date belongs to\"\"\"\n",
    "    if d.month<=3: return \"Q1\"\n",
    "    if d.month<=6: return \"Q2\"\n",
    "    if d.month<=9: return \"Q3\"\n",
    "    return \"Q4\"\n",
    "\n",
    "def getSemester(d):\n",
    "    \"\"\"Detects which semester of the year date belongs to\"\"\"\n",
    "    if d.month<=6: return \"S1\"\n",
    "    return \"S2\"\n",
    "\n",
    "def is_holiday(d):\n",
    "    if len(df_holidays2[(df_holidays2['Day']==d.day) & (df_holidays2['Month']==d.month)])>0: return 'Holiday'\n",
    "    else: return 'Not Holiday'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of date dimension primary key in format 'ddMMyyyy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings['DataRegisto'][0], date_pk(df_listings['DataRegisto'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Preprocessing *feriados_nacionais.csv* data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_file_path = '../data/feriados_nacionais.csv'\n",
    "df_holidays = pd.read_csv(holiday_file_path)\n",
    "df_holidays.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the file in order to get a clean dataframe with the holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_dates = df_holidays.iloc[:,1].values[:-1]\n",
    "holidays_dates = [datetime.strptime(d, \"%m/%d/%Y\") for d in holidays_dates]\n",
    "\n",
    "holidays_days = [d.day for d in holidays_dates]\n",
    "holidays_months = [d.month for d in holidays_dates]\n",
    "\n",
    "df_holidays2 = pd.DataFrame(np.stack((holidays_days,holidays_months), axis = 1),columns=['Day','Month'])\n",
    "df_holidays2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in this dataframe will be used in order to fed the 'Holiday' column of the dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Processing the _df_listings_ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pks = [date_pk(d) for d in df_listings['DataRegisto']]\n",
    "days = [d.day for d in df_listings['DataRegisto']]\n",
    "weeks = [week_of_month(d) for d in df_listings['DataRegisto']]\n",
    "week_days = [week_day(d) for d in df_listings['DataRegisto']]\n",
    "week_ends = [date_weekend(d.weekday()) for d in df_listings['DataRegisto']]\n",
    "months = [d.month for d in df_listings['DataRegisto']]\n",
    "seasons = [date_season(d.month) for d in df_listings['DataRegisto']]\n",
    "quarters = [getQuarter(d) for d in df_listings['DataRegisto']]\n",
    "semesters = [getSemester(d) for d in df_listings['DataRegisto']]\n",
    "holidays = [is_holiday(d) for d in df_listings['DataRegisto']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing each date in the dataframe and building a list of values for each attribute, we merge them together in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['date_id','day','week', 'month','year','season','weekend','weekday','quarter','semester','holiday']\n",
    "df_date_dimension = pd.DataFrame(np.stack((pks,days,weeks, months,years, seasons,week_ends,week_days,quarters,semesters,holidays),axis=-1), columns = columns)\n",
    "df_date_dimension.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_dimension.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removal of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_dimension = df_date_dimension.drop_duplicates(subset=['date_id'])\n",
    "df_date_dimension.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Processing the _df_calendar_ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pks2 = [date_pk(d) for d in df_calendar2['date']]\n",
    "days2 = [d.day for d in df_calendar2['date']]\n",
    "weeks2 = [week_of_month(d) for d in df_calendar2['date']]\n",
    "week_days2 = [week_day(d) for d in df_calendar2['date']]\n",
    "week_ends2 = [date_weekend(d.weekday()) for d in df_calendar2['date']]\n",
    "months2 = [d.month for d in df_calendar2['date']]\n",
    "seasons2 = [date_season(d.month) for d in df_calendar2['date']]\n",
    "quarters2 = [getQuarter(d) for d in df_calendar2['date']]\n",
    "semesters2 = [getSemester(d) for d in df_calendar2['date']]\n",
    "years2 = [d.year for d in df_calendar2['date']]\n",
    "holidays2 = [is_holiday(d) for d in df_calendar2['date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the two dataframes (dates needed both for availability and listing facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_date_dimension_insert = pd.concat([pd.DataFrame(np.stack((pks2,days2,weeks2,months2,years2,seasons2,week_ends2,week_days2,quarters2,semesters2,holidays2),axis=-1), columns = columns),df_date_dimension])\n",
    "df_date_dimension_insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Insert data into DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_connection import dbconnection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_table = \"\"\"\n",
    "DROP TABLE IF EXISTS Date;\n",
    "\"\"\"\n",
    "\n",
    "create_table = \"\"\"\n",
    "CREATE TABLE Date (\n",
    "    DATE_ID INT PRIMARY KEY NOT NULL,\n",
    "    DAY INT NOT NULL,\n",
    "    WEEK INT NOT NULL,\n",
    "    MONTH INT NOT NULL,\n",
    "    YEAR INT NOT NULL,\n",
    "    SEASON VARCHAR(10) CHECK(SEASON in ('Spring', 'Summer', 'Autumn', 'Winter')) NOT NULL,\n",
    "    WEEKEND VARCHAR(11) CHECK (WEEKEND in ('Weekend', 'Work Day')) NOT NULL,\n",
    "    WEEKDAY VARCHAR(10) CHECK (WEEKDAY IN ('Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday')) NOT NULL,\n",
    "    QUARTER VARCHAR(2) CHECK(QUARTER IN ('Q1','Q2','Q3','Q4')) NOT NULL,\n",
    "    SEMESTER VARCHAR(2) CHECK(SEMESTER IN ('S1','S2')) NOT NULL,\n",
    "    HOLIDAY VARCHAR(11) CHECK (HOLIDAY in ('Holiday', 'Not Holiday')) NOT NULL\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excuteSingleSQLstatement(sql, host, database, user, password,gssencmode):\n",
    "    conn = pg.connect(host=host,database=database, user=user, password=password,sslmode = sslmode,gssencmode=gssencmode)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    cur.close()\n",
    "    conn.commit()\n",
    "    conn.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute just the first time the notebook runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excuteSingleSQLstatement(delete_table, dbconnection.server_host, dbconnection.dbname, dbconnection.dbusername, dbconnection.dbpassword,gssencmode = dbconnection.gssencmode)\n",
    "#excuteSingleSQLstatement(create_table, dbconnection.server_host, dbconnection.dbname, dbconnection.dbusername, dbconnection.dbpassword,gssencmode = dbconnection.gssencmode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to query table and convert it to pandas dataframe\n",
    "def query_table(conn, table_name):\n",
    "    \"\"\"Returns DataFrame with queried database table\"\"\"\n",
    "    sql = \"select * from {};\".format(table_name)\n",
    "    #return dataframe\n",
    "    return sqlio.read_sql_query(sql, conn)\n",
    "\n",
    "# for this function to run, the dataframes must have the same columns, in the same order\n",
    "def get_data_to_insert(df_etl, df_sql,pk):\n",
    "    \"\"\"Returns data valid for insertion in dimension from a new ETL-processed DataFrame\"\"\"\n",
    "    df_insert = df_etl[-df_etl[pk].astype(int).isin(df_sql[pk].astype(int))].dropna(how = 'all')\n",
    "    df_insert = df_insert.drop_duplicates(subset=[pk], keep=False)\n",
    "    return df_insert\n",
    "\n",
    "# function for bulk insert\n",
    "def insert_data(df, table_name, conn):\n",
    "    \"\"\"Inserts selected data into dimension table in database\"\"\"\n",
    "    df_columns = list(df)\n",
    "    columns = \",\".join(df_columns)\n",
    "    values = \"VALUES({})\".format(\",\".join([\"%s\" for _ in df_columns])) \n",
    "    insert_stmt = \"INSERT INTO {} ({}) {}\".format(table_name,columns,values)\n",
    "    success = True\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        psycopg2.extras.execute_batch(cursor, insert_stmt, df.values)\n",
    "        conn.commit()\n",
    "        success = True\n",
    "    except pg.DatabaseError as error:\n",
    "        success = False\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    return success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get date dimension data from postgres db server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "df_date_dimension_sql = query_table(conn, 'date')\n",
    "conn.close()\n",
    "df_date_dimension_sql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarantee that duplicates will be not be inserted, by getting just the data in dataframe with the ETL output (df_date_dimension_insert) that is not in df_sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_insert = get_data_to_insert(df_date_dimension_insert,df_date_dimension_sql,'date_id')\n",
    "df_insert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert data into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df_insert) > 0:\n",
    "    table_name = 'date'\n",
    "    conn = psycopg2.connect(host = dbconnection.server_host,database = dbconnection.dbname, user = dbconnection.dbusername,password = dbconnection.dbpassword,sslmode=dbconnection.sslmode,gssencmode=dbconnection.gssencmode)\n",
    "    success = insert_data(df_insert,table_name, conn)\n",
    "    conn.close()\n",
    "    if success == True: print('Data inserted succefully')\n",
    "else: print('No data to insert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataframe that saves the mapping between the listing_id and date_id. This will be used to load the listings fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_date = df_listings[['id','DataRegisto']]\n",
    "df_listings_date['date_id'] = [date_pk(d) for d in df_listings_date['DataRegisto']]\n",
    "df_listings_date = df_listings_date.drop(['DataRegisto'], axis=1)\n",
    "df_listings_date = df_listings_date.rename(columns={'id':'listing_id'})\n",
    "df_listings_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save csv to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_listings_date.to_csv('../processed_dt/df_listings_date.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same process with calendar data, that contains the data to feed the availability fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calendar.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this project we will just load the availability fact table with 20 000 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_availability_date = df_calendar[:20000][['listing_id','date']]\n",
    "df_availability_date['date_id'] = [date_pk(d) for d in df_availability_date['date']]\n",
    "df_availability_date = df_availability_date.drop(['date'], axis=1)\n",
    "df_abailability_date = df_availability_date.drop_duplicates(subset=['listing_id','date_id'])\n",
    "df_abailability_date.shape, df_abailability_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abailability_date.to_csv('../processed_dt/df_availability_date.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
