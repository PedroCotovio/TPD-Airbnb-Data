{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import psycopg2 as pg\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import psycopg2.extras\n",
    "\n",
    "# set environment variable for psycog2 (for some systems)\n",
    "os.environ[\"PGGSSENCMODE\"] = \"disable\"\n",
    "\n",
    "# get the stored passwords\n",
    "f=open(\"credentials_david.txt\", \"rt\")\n",
    "pwd=f.readline().strip()  #di   password\n",
    "hpwd=f.readline().strip() #home password\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# di test: WORKING\n",
    "#host_di = \"appserver-01.alunos.di.fc.ul.pt\"\n",
    "#db_di = \"tpd012\"\n",
    "#user_di = \"tpd012\"\n",
    "\n",
    "#conn = pg.connect(host = host_di,\n",
    "#                  database = db_di,\n",
    "#                  user = user_di,\n",
    "#                  password = pwd)\n",
    "#conn.close()\n",
    "\n",
    "# local test: WORKING\n",
    "host_local = \"localhost\"\n",
    "db_local = \"tpd012\"\n",
    "user_local = \"postgres\"\n",
    "\n",
    "conn = pg.connect(host = host_local,\n",
    "                  database = db_local,\n",
    "                  user = user_local,\n",
    "                  password=hpwd)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def run_sql_command(sql, host, database, user, password):\n",
    "    \"\"\"Executes a single SQL statement from a string variable and the database credentials\"\"\"\n",
    "    conn = pg.connect(host = host,\n",
    "                      database = database,\n",
    "                      user = user,\n",
    "                      password = password)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql)\n",
    "    cur.close()\n",
    "    conn.commit()\n",
    "    conn.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intTryParse(value):\n",
    "    \"\"\"Parse a string to an integer\"\"\"\n",
    "    try:\n",
    "        a = int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_al_file_path = '../data/listings_al.csv'\n",
    "df_listings_al = pd.read_csv(listings_al_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The defined _'Property'_ dimension table in __Phase I__ is as follows:\n",
    "\n",
    "<img src=\"property_schema.png\" width=\"150\" align=\"center\"/>\n",
    "\n",
    "# __CHANGE SCHEMA__\n",
    "\n",
    "<img src=\"PropertyETL.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_null_rows(df,columns):\n",
    "    \"\"\"Removes all records with any empty cells from input DataFrame\"\"\"\n",
    "    processed_df = df[columns].copy()\n",
    "    total_rows = processed_df.shape[0]\n",
    "    delete_rows = []\n",
    "    \n",
    "    if processed_df.isnull().values.any(): # if there are any null values in DataFrame, process DataFrame\n",
    "        for index, row in enumerate(processed_df.itertuples(), start = 0):\n",
    "            if (pd.Series(row).isnull().values.any()): # if row has any null value\n",
    "                delete_rows.append(index) # add row index to delete list\n",
    "    \n",
    "    processed_df.drop(df.index[delete_rows], inplace = True) # delete rows fr\n",
    "    processed_df = processed_df.reset_index().drop('index', axis = 1)\n",
    "    print('DataFrame contains {} rows. Deleted {} rows ({}% of total rows)'.format(processed_df.shape[0], len(delete_rows), round(len(delete_rows)*100/total_rows, 2)))\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns = ['id',\n",
    "              'property_type',\n",
    "              'room_type',\n",
    "              'accommodates',\n",
    "              'bathrooms',\n",
    "              'bedrooms',\n",
    "              'beds',\n",
    "              'bed_type']\n",
    "\n",
    "df_property = delete_null_rows(df_listings_al,df_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `df_property`, we create `df_conv_fact`, which will be the table containing a record for each fact ID converted to the dimension format. `property_dimension` will be the _de facto_ property dimension from which we can correspond each fact ID record to the appropriate dimension foreign key according to its set of attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _'property_type_category'_\n",
    "\n",
    "We start by creating _'property_type_category'_, a set of four categories from _'property_type'_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_property_type_category(value):\n",
    "# change 'property_type' to four categories, exclude exceptions\n",
    "    if value in [\"Apartment\", \"Serviced apartment\", \"Aparthotel\",\"Loft\"]:\n",
    "        return 'Apartment'\n",
    "    if value in [\"House\",\"Townhouse\",\"Villa\",\"Dome house\",\"Vacation home\",\"Lighthouse\",\"Casa particular (Cuba)\",\"Tiny house\",\"Farm stay\",\"Cottage\"]:\n",
    "        return 'House'\n",
    "    if value in [\"Guesthouse\",\"Guest suite\"]:\n",
    "        return 'Guesthouse'\n",
    "    if value in [\"Hostel\",\"Bed and breakfast\",\"Boutique hotel\",\"Hotel\"]:\n",
    "        return 'Hotel/Hostel'\n",
    "    return None # for the case of missing or invalid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _'room_type'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This attribute is already defined in satisfactory categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _'accommodates'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accommodates(value):\n",
    "# change 'property_type' to four categories, exclude exceptions\n",
    "    if value in [1,2]: return 'Up to 2 guests'\n",
    "    if value in [3,4]: return 'Up to 4 guests'\n",
    "    if value in [5,6]: return 'Up to 6 guests'\n",
    "    return 'Up to 7 guests or more' # no missing values in this column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _'bathrooms'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bathrooms(value):\n",
    "# change 'bathrooms' to five categories, exclude exceptions\n",
    "    if value < 0.5: return 'No bathrooms'\n",
    "    if value < 1.5: return '1 bathroom'\n",
    "    if value < 2.5: return '2 bathrooms'\n",
    "    if value < 3.5: return '3 bathrooms'\n",
    "    if value >= 3.5: return '4+ bathrooms'\n",
    "    return None # for the case of missing or invalid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _'bedrooms'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bedrooms(value):\n",
    "# change 'bedrooms' to five categories, exclude exceptions\n",
    "    if value == 0: return 'T0'\n",
    "    if value == 1: return 'T1'\n",
    "    if value == 2: return 'T2'\n",
    "    if value == 3: return 'T3'\n",
    "    if value >= 4: return 'T4+'\n",
    "    return None # for the case of missing or invalid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _'beds'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beds(value):\n",
    "# change 'beds' to five categories, exclude exceptions\n",
    "    if value == 0: return 'No beds'\n",
    "    if value == 1: return '1 bed'\n",
    "    if value == 2: return '2 beds'\n",
    "    if value == 3: return '3 beds'\n",
    "    if value >= 4: return '4+ beds'\n",
    "    return None # for the case of missing or invalid values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Creating the dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = {}\n",
    "\n",
    "dimension['ID'] = [value for value in df_property['id']]\n",
    "dimension['property_type_category'] = [get_property_type_category(value) for value in df_property['property_type']]\n",
    "dimension['property_type'] = [value for value in df_property['property_type']]\n",
    "dimension['room_type'] = [value for value in df_property['room_type']]\n",
    "dimension['accommodates'] = [get_accommodates(value) for value in df_property['accommodates']]\n",
    "dimension['bathrooms'] = [get_bathrooms(value) for value in df_property['bathrooms']]\n",
    "dimension['bedrooms'] = [get_bedrooms(value) for value in df_property['bedrooms']]\n",
    "dimension['beds'] = [get_beds(value) for value in df_property['beds']]\n",
    "dimension['bed_type'] = [value for value in df_property['bed_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv_facts = pd.DataFrame(dimension)\n",
    "df_conv_facts.index += 1\n",
    "df_conv_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv_facts = delete_null_rows(df_conv_facts,df_conv_facts.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df_property_dimension\n",
    "property_dimension = df_conv_facts[['property_type_category','property_type','room_type','accommodates','bathrooms','bedrooms','beds','bed_type']].drop_duplicates().copy()\n",
    "property_dimension = property_dimension.reset_index().drop('index', axis = 1)\n",
    "property_dimension.index += 1\n",
    "\n",
    "property_dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Attributing dimension keys to facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having our dimension DataFrame `property_dimension` ready, we can now use our converted listings table, `df_conv_facts` (which contains Property dimension attributes for all valid facts), to assign each fact its corresponding Property dimension foreign key.\n",
    "\n",
    "<img src=\"PropertyETL2.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change df_property_dimension to property dimension ID\n",
    "FK = []\n",
    "for i, row in enumerate(df_conv_facts.itertuples(), start = 1): \n",
    "    row = list(row)\n",
    "    for j, dim_row in enumerate(property_dimension.itertuples(), start = 1):\n",
    "        dim_row = list(dim_row) \n",
    "        if (row[2:] == dim_row[1:]): # if fact attributes are equal to dimension record\n",
    "            FK.append(j)\n",
    "            break # leaves nested loop if match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv_facts['Property'] = FK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_conv_facts = df_conv_facts[['ID','Property']]\n",
    "df_conv_facts.index += 1\n",
    "df_conv_facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This table can then be merged with all other corresponding tables for the remaining dimensions to produce each fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. Creating the _Property_ dimension table in SQL\n",
    "\n",
    "The SQL statement to create the table is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_table = \"\"\"\n",
    "DROP TABLE IF EXISTS Property;\n",
    "\"\"\"\n",
    "\n",
    "create_table = \"\"\"\n",
    "CREATE TABLE Property (\n",
    "    PROPERTY_ID SERIAL PRIMARY KEY NOT NULL,\n",
    "    PROPERTY_TYPE_CATEGORY VARCHAR(30) CHECK (PROPERTY_TYPE_CATEGORY in ('Apartment', 'Guesthouse', 'House', 'Hotel/Hostel')) NOT NULL,\n",
    "    PROPERTY_TYPE VARCHAR(30) CHECK (PROPERTY_TYPE in ('Apartment','Serviced apartment','Aparthotel','Loft','House','Townhouse','Villa','Dome house','Vacation home','Lighthouse','Casa particular (Cuba)','Tiny house','Farm stay','Cottage','Guesthouse','Guest suite','Hostel','Bed and breakfast','Boutique hotel','Hotel')) NOT NULL,\n",
    "    ROOM_TYPE VARCHAR(30) CHECK (ROOM_TYPE in ('Entire home/apt', 'Private room', 'Hotel room', 'Shared room')) NOT NULL,\n",
    "    ACCOMMODATES VARCHAR(30) CHECK (ACCOMMODATES in ('Up to 2 guests','Up to 4 guests','Up to 6 guests','Up to 7 guests or more')) NOT NULL,\n",
    "    BATHROOMS VARCHAR(30) CHECK (BATHROOMS in ('No bathrooms','1 bathroom','2 bathrooms','3 bathrooms','4+ bathrooms')) NOT NULL,\n",
    "    BEDROOMS VARCHAR(10) CHECK (BEDROOMS in ('T0','T1','T2','T3','T4+')) NOT NULL,\n",
    "    BEDS VARCHAR(10) CHECK (BEDS in ('No beds','1 bed','2 beds','3 beds','4+ beds')) NOT NULL,\n",
    "    BED_TYPE VARCHAR(30) CHECK (BED_TYPE in ('Real Bed', 'Pull-out Sofa', 'Futon', 'Couch', 'Airbed')) NOT NULL\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `create_table` command contains the integrity constraints essential for modelling the dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating table from scratch\n",
    "\n",
    "run_sql_command(delete_table, host_local, db_local, user_local, hpwd)\n",
    "run_sql_command(create_table, host_local, db_local, user_local, hpwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading DataFrame data to dimension in database\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"postgres://{user}:{password}@{host}:5432/{database}\".format(user = user_local,\n",
    "                                                                                    password = hpwd,\n",
    "                                                                                    host = host_local,\n",
    "                                                                                    database = db_local))\n",
    "property_dimension.to_sql('property',\n",
    "                          con = engine,\n",
    "                          if_exists = 'append',\n",
    "                          index = True,\n",
    "                          index_label = 'property_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. Adding new facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the pipeline diagram, when new data is obtained, the DataFrame's columns of interest are converted to the _Property_ dimension format are compared with the dimension table for attributes and ID. In this way, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to query table and convert it to pandas dataframe\n",
    "def query_table(conn, table_name):\n",
    "    sql = \"select * from {};\".format(table_name)\n",
    "    #return dataframe\n",
    "    return pd.read_sql_query(sql, conn)\n",
    "\n",
    "# for this function to run, the dataframes must have the same columns, in the same order\n",
    "def get_data_to_insert(df_etl, df_sql):\n",
    "    return df_etl[~df_etl.isin(df_sql)].dropna(how = 'all')\n",
    "\n",
    "# function for bulk insert\n",
    "def insert_data(df, table_name, conn):\n",
    "    df_columns = list(df)\n",
    "    columns = \",\".join(df_columns)\n",
    "    values = \"VALUES({})\".format(\",\".join([\"%s\" for _ in df_columns])) \n",
    "    insert_stmt = \"INSERT INTO {} ({}) {}\".format(table_name,columns,values)\n",
    "    success = True\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        psycopg2.extras.execute_batch(cursor, insert_stmt, df.values)\n",
    "        conn.commit()\n",
    "        success = True\n",
    "    except pg.DatabaseError as error:\n",
    "        success = False\n",
    "        print(error)\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "    return success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "query_table() got an unexpected keyword argument 'index_col'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4714fdb1bad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                         password = hpwd)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mproperty_dimension_sql\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'property'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'property_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: query_table() got an unexpected keyword argument 'index_col'"
     ]
    }
   ],
   "source": [
    "# retrieve Property dimension table\n",
    "conn = pg.connect(host = host_local,\n",
    "                        database = db_local,\n",
    "                        user = user_local,\n",
    "                        password = hpwd)\n",
    "\n",
    "property_dimension_sql = query_table(conn, 'property')\n",
    "\n",
    "conn.close()\n",
    "\n",
    "property_dimension_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with dummy_data\n",
    "new_data_file_path = '../data/MOCK_DATA.csv'\n",
    "df_new_data = pd.read_csv(new_data_file_path)\n",
    "df_new_data.index += 1\n",
    "\n",
    "# dummy data is already converted to dimension attributes; that preprocessing is easy to replicate\n",
    "df_new_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
