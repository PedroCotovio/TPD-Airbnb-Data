{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tecnologias de Processamento de dados - 2019/2020\n",
    "\n",
    "## Phase II - Group 12\n",
    "\n",
    "\n",
    "|   Student      | Student ID |  Contribution in hours |\n",
    "|----------------|------------|----------------|\n",
    "| Beatriz Lima   |    49377   |   |\n",
    "| David Almeida  |    54120   |   |\n",
    "|Jo√£o Castanheira|    55052   |   |\n",
    "| Pedro Cotovio  |    55053   |   |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main datasets used in this warehouse are:\n",
    "\n",
    "- http://insideairbnb.com/get-the-data.html for Lisbon, Portugal. - listings.csv\n",
    "- https://dadosabertos.turismodeportugal.pt/datasets/alojamento-local) - Alojamento_Local.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/air/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3051: DtypeWarning: Columns (61,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "listings_file_path = '../data/airbnb/listings.csv'\n",
    "al_file_path = '../data/Alojamento_Local.csv'\n",
    "df_al = pd.read_csv(al_file_path)\n",
    "df_listings = pd.read_csv(listings_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge _df_listings_ with _alojamento_local.csv_\n",
    "\n",
    "In order to enrich the main dataset, we can cross it with the dataset from Registo Nacional de Alojamento Local (RNAL) to obtain further information regarding each listing's property, as well as refine already available data, particularly in the case of location data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intTryParse(value):\n",
    "    \"\"\"Tries to parse string to an integer\"\"\"\n",
    "    try:\n",
    "        a = int(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/air/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# get only listings where \n",
    "df_listings_with_license = df_listings[(~df_listings['license'].isnull()) #'license' is not null\n",
    "                                        & (df_listings['license'] != 'Exempt')] # && != 'Exempt'\n",
    "\n",
    "# string replace\n",
    "df_listings_with_license['NrRNAL'] = [s.replace('/AL','').replace('.','') # remove '/AL' and '.' from code\n",
    "                                      for s in df_listings_with_license['license']]\n",
    "\n",
    "# get only records where license nr can be converted to int \n",
    "df_listings_with_license = df_listings_with_license[[intTryParse(s) # if code can be converted to int\n",
    "                                                     for s in df_listings_with_license['NrRNAL']]] # keep it\n",
    "\n",
    "# convert NrRNAL to int before merge the two dataframes\n",
    "df_listings_with_license['NrRNAL'] = df_listings_with_license['NrRNAL'].astype(np.int64) # convert code to int\n",
    "\n",
    "# inner join two dataframes\n",
    "df_result = pd.merge(df_listings_with_license, df_al, how='inner', on='NrRNAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the intersection of the two files to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 17168\n"
     ]
    }
   ],
   "source": [
    "listings__al_file_path = '../data/listings_al.csv'\n",
    "df_result.to_csv(listings__al_file_path,index=False)\n",
    "print('Dataset size: {}'.format(len(df_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dimensions and facts tables of the data warehouse\n",
    "\n",
    "+ Define and model them in SQL\n",
    "+ Identify hierarchies and fact granularity\n",
    "+ Create the dimensions and facts tables in the DBMS (postgreSQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General schema\n",
    "\n",
    "![Star schema](../schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define an ETL workflow\n",
    "\n",
    "+ Identify all data sources for all dimensions. Add URL links to all data that should be available. If not public data, point to dropbox files, Google drive, or whatever\n",
    "+ For each dimension show the code used for modeling, filtering and inserting data\n",
    "+ Describe the process for inserting facts data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ETL workflow is defined in separate notebooks for each dimension:\n",
    "\n",
    " - ETL_Property.ipynb\n",
    " - ETL_Host.ipynb\n",
    " - ETL_Review.ipynb\n",
    " - ETL_Date.ipynb\n",
    " - ETL_Location.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Critical assessment of the work\n",
    "+ Describe potential issues with the ETL procedure used\n",
    "+ Compare your schema to the one previously defined in phase I\n",
    "+ Discuss the issues for updating the data warehouse with novel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
