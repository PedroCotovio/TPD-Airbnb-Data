{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Mining / Prospecção de Dados\n",
    "### Sara C. Madeira and André Falcão\n",
    "#### Pattern Mining I\n",
    "\n",
    "### 0. Getting Started\n",
    "\n",
    "In this notebook, we use Python 3, Jupyter Notebook and MLxtend. MLxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks developed by Sebastian Raschka, which uses Pandas, NumPy, Scikit-learn, Matplotlib and SciPy.\n",
    "\n",
    "In the lab and at home you should have the latest version of Anaconda, which already installs Python 3, Jupyter, Scikit-learn, Pandas, NumPy, Scikit-learn, Matplotlib and SciPy.\n",
    "\n",
    "MLxtend is not installed with Anaconda. Proceed as follows:\n",
    "\n",
    "    In your computer/if you have permissions you can install MLxtend.\n",
    "\n",
    "MLxtend is supported in Anaconda (https://anaconda.org/conda-forge/mlxtend). To install this package with conda, run the following in command line and follow the instructions:\n",
    "\n",
    "conda install -c conda-forge mlxtend.\n",
    "\n",
    "After this you should be ready to start.\n",
    "\n",
    "OR\n",
    "\n",
    "    In the LAB/if you do not have permissions you have to keep the mlxtend folder in your working directory..\n",
    "\n",
    "If you are wondering why are we not using Scikit-Learn ?\n",
    "\n",
    "Scikit-learn does not have implementations of pattern mining algorithms.\n",
    "\n",
    "## 1. Frequent Pattern Mining and Association Rule Mining in MLxtend\n",
    "\n",
    "In this section we follow closely the examples on generating frequent Itemsets via Apriori Algorithm and Association Rules Generation from Frequent Itemsets provided in the documentation of MLxtend by Sebastian Raschka.\n",
    "\n",
    "### 1.1. Preprocess Dataset\n",
    "\n",
    "Consider the previous example of a set of transactions (baskets) containing a set products (items) bought at a given supermarket.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction data (market baskets)\n",
    "transactions = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]\n",
    "transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we are not so concerned with representation as we are going to use an external library to process that information in new data structure.\n",
    "\n",
    "The mlextend library understands transactions represented as list of lists and contructs a special array to process it.\n",
    "We also are going to use pandas to have a novel look at our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import  TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Apriori implementation at MLxtend receives a binary database, thus the first step is to transform the transactions database into a binary database as an array, where each line iis a transaction, each column j is an item (product) and 1 means at position ij means item j appears at transaction i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute binary database\n",
    "tr_enc = TransactionEncoder()\n",
    "trans_array = tr_enc.fit(transactions).transform(transactions)\n",
    "binary_database = pd.DataFrame(trans_array, columns=tr_enc.columns_)\n",
    "binary_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Compute Frequent Itemsets using Apriori\n",
    "\n",
    "We can now input the binary database to apriori and compute frequent itemsets. Consider a minimum support of 60%, which in this case means an item is frequent if it appears in at least 3 transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(binary_database, min_support=0.6)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, apriori returns the column indexes of the items, which may be useful in downstream operations, such as association rule mining. For better readability, we can set use_colnames=True to convert these integer values into the respective item names.\n",
    "\n",
    "The advantage of working with pandas DataFrames is that we can use its convenient features to filter the results. For instance, let us assume we decide that after all we are only interested in itemsets of length 2 that have a support of at least 80 percent. Given that we already have the frequent itemsets and their support, we can add a new column that stores the length of each itemset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute itemsets with min_support = 60% with item names\n",
    "frequent_itemsets = apriori(binary_database, min_support=0.6, use_colnames=True)\n",
    "# Add new column length\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now filter the results based on the desired support and pattern length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter using support and pattern length\n",
    "\n",
    "frequent_itemsets = frequent_itemsets[ (frequent_itemsets['support'] >= 0.8) & (frequent_itemsets['length'] == 2)]\n",
    "frequent_itemsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we already knew that we were only intested in patterns with at least 80% support it was more efficient to run the Apriori algorithm already with this minimum support value and then filter the results based only on pattern length. Can you do this ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute frequent itemsets with min_support=0.8\n",
    "# filter using pattern length\n",
    "# add new column length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.3. Generate Association Rules from Frequent Itemsets\n",
    "\n",
    "The first step in association rule mining is to find the frequent itemsets. In this context, we can now generate association rules from the frequent itemsets first discovered using Apriori. In what follows, we follow closely the examples in Association Rules Generation from Frequent Itemsets.\n",
    "\n",
    "The method generate_rules takes dataframes of frequent itemsets as produced by the apriori function in mlxtend.association. To demonstrate the usage of generate_rules, we first create a pandas DataFrame of frequent itemsets as generated by the apriori function.\n",
    "\n",
    "The generate_rules function allows you to: 1) specify your metric of interest and (2) - specify the according threshold. Currently implemented measures are confidence and lift.\n",
    "\n",
    "Consider we are interesting in rules derived from the frequent itemsets only if the level of confidence is above the 90 percent threshold (min_threshold=0.9):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first retrieve the original itemsets with 60% support\n",
    "frequent_itemsets = apriori(binary_database, min_support=0.6, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "# Generate association rules with confidence >= 60%\n",
    "\n",
    "all_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.6)\n",
    "all_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we are interested in rules fulfilling a different interest metric, we can simply adjust the parameters. For example, in case we are only interested in rules that have a lift score of >= 1.2, we would do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules with lift >= 1.2\n",
    "good_rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "good_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, on the other hand, we are interested in rules with confidence above 90% and lift >= 1.2, we can generate the rules using the confidence as metric and then filter using the lift, or vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules with confidence >= 90%\n",
    "all_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.9)\n",
    "# Filter association rules using lift\n",
    "new_rules = all_rules[rules['lift'] >= 1.2]\n",
    "new_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1.4. Small Exercise\n",
    "\n",
    "Consider the set of transactions below taken from from Han, Kamber and Pei, Chapter 6 and used as example in the theoretical lesson. Create and preprocess the dataset and then use the functions apriori and association_rules to generate, respectively, frequent patterns with different values for minimum support, and association rules with different values for confidence and lift.\n",
    "\n",
    "AllElectronics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Market Basket Analysis in a Real Dataset using MLxtend\n",
    "\n",
    "We will now use the dataset groceries.csv containing 9835 transactions (baskets) and 169 items (products) collected from a supermarket and download here.\n",
    "\n",
    "\n",
    "### 2.1. Preprocess Dataset\n",
    "\n",
    "Take a look at the dataset by opening the .csv file. We will use the function load_transactions below to load the dataset into the format used in the examples above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transactions (csv_file):\n",
    "# input: csv file with one transaction per line,\n",
    "#       where transactions may have a different number of items\n",
    "# output: matrix where each row is a vector of items (transaction)\n",
    "# author: Sara C. Madeira, Oct 2017  \n",
    "    lines = open(csv_file, 'r').readlines()\n",
    "    transactions_matrix = []\n",
    "    for l in lines:\n",
    "        l = l.rstrip('\\n')\n",
    "        transaction = l.split(',')\n",
    "        transactions_matrix.append(transaction)\n",
    "    return transactions_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transaction from file groceries.csv\n",
    "transactions = load_transactions('groceries.csv')\n",
    "transactions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of transactions\n",
    "len(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute binary database (transactions X products )\n",
    "tr_enc = TransactionEncoder()\n",
    "trans_array = tr_enc.fit(transactions).transform(transactions)\n",
    "binary_database = pd.DataFrame(trans_array, columns=tr_enc.columns_)\n",
    "binary_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2. Compute Frequent Itemsets\n",
    "\n",
    "Let us use apriori to compute the frequent itemsets. Note that due to the number of transactions and different items the computations might not be instantaneous as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute itemsets min_support = 20%\n",
    "frequent_itemsets = apriori(binary_database, min_support=0.2, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute itemsets min_support = 10%\n",
    "frequent_itemsets = apriori(binary_database, min_support=0.1, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute itemsets min_support = 5%\n",
    "frequent_itemsets = apriori(binary_database, min_support=0.05, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute itemsets min_support = 1%\n",
    "frequent_itemsets = apriori(binary_database, min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new column length\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter using pattern length = 2\n",
    "frequent_2_itemsets = frequent_itemsets[frequent_itemsets['length'] == 2]\n",
    "frequent_2_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter using pattern length = 3\n",
    "frequent_3_itemsets = frequent_itemsets[frequent_itemsets['length'] == 3]\n",
    "frequent_3_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Generate Association Rules from Frequent Itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute itemsets min_support = 1%\n",
    "frequent_itemsets = apriori(binary_database, min_support=0.01, use_colnames=True)\n",
    "print(len(frequent_itemsets))\n",
    "# Compute association rules with 80% confidence\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.4)\n",
    "#pd.options.display.max_rows=None\n",
    "rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute association rules with 50% confidence\n",
    "rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.5)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Self Experiences with Frequent Pattern Mining and Association Rule Mining in other Real Problems and Datasets\n",
    "\n",
    "In the webpage of SPMF - An Open-Source Data Mining Library you can find a list of Datasets for Frequent Itemset mining / Association Rule Mining, an interesting collection of already preprocessed real-life datasets collected from several machine learning/data mining data repositories and competitions, such as Kaggle - The Home of Data Science & Machine Learning, KDD Cup - The Data Mining and Knowledge Discovery competition and UCI-Machine Learning Repository.\n",
    "\n",
    "Choose some and have fun !\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
